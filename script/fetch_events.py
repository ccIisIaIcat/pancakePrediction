#!/usr/bin/env python3
"""
PancakeSwap Prediction åˆçº¦äº‹ä»¶è·å–è„šæœ¬
æ”¯æŒåˆ†æ‰¹è·å–å¤šå¤©çš„å†å²äº‹ä»¶æ•°æ®
"""

import json
import requests
import time
from typing import List, Dict, Any
from datetime import datetime

# é…ç½®
# BSC å…¬å¼€èŠ‚ç‚¹åˆ—è¡¨ï¼ˆæŒ‰æ¨èé¡ºåºï¼‰ï¼š
# 1. https://bsc-dataseed.binance.org (å®˜æ–¹)
# 2. https://bsc-dataseed1.binance.org (å®˜æ–¹å¤‡ç”¨)
# 3. https://bsc-dataseed2.binance.org (å®˜æ–¹å¤‡ç”¨)
# 4. https://bsc.publicnode.com (ç¤¾åŒºèŠ‚ç‚¹)
# 5. https://bsc-rpc.publicnode.com (ç¤¾åŒºèŠ‚ç‚¹)
RPC_URL = "https://bsc-dataseed.binance.org"
OUTPUT_FILE = "pancake_events.json"

# åˆçº¦åœ°å€
CONTRACTS = [
    "0x18B2A687610328590Bc8F2e5fEdDe3b582A49cdA",  # PancakeSwap Prediction V2 (BNB)
    "0x48781a7d35f6137a9135Bbb984AF65fd6AB25618",
    "0x7451F994A8D510CBCB46cF57D50F31F188Ff58F5"
]


# äº‹ä»¶ç­¾åå“ˆå¸Œ (Topic0)
EVENT_TOPICS = {
    "BetBull": "0x438122d8cff518d18388099a5181f0d17a12b4f1b55faedf6e4a6acee0060c12",
    "BetBear": "0x0d8c1fe3e67ab767116a81f122b83c2557a8c2564019cb7c4f83de1aeb1f1f0d",
    "Claim": "0x34fcbac0073d7c3d388e51312faf357774904998eeb8fca628b9e6f65ee1cbf7",
    "LockRound": "0x85b533c0fa284d94993934e2b570c1e9b3b7d0bdb3e0ce92e65c26fd46f481a2",
    "StartRound": "0x0bb59eceb12f1bdd2f0b3c6e68bc76f3c93d328d251b1fca62a51f62f28c90a4",
    "EndRound": "0x070f615e3c25ace7a92f3a2f441c8d41bbfdafc6d641e2c544be47c0cd870e91"
}

# BSC å¹³å‡å‡ºå—æ—¶é—´çº¦ 3 ç§’ï¼Œä¸€å¤©çº¦ 28800 ä¸ªåŒºå—
BLOCKS_PER_DAY = 28800
# æ¯æ¬¡æŸ¥è¯¢çš„æœ€å¤§åŒºå—æ•°ï¼ˆé¿å…èŠ‚ç‚¹é™åˆ¶ï¼‰
MAX_BLOCKS_PER_QUERY = 1000
# è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- é¿å…è§¦å‘é™æµ
REQUEST_DELAY = 0.5
# å¤±è´¥é‡è¯•æ¬¡æ•°ï¼ˆè®¾ä¸ºå¾ˆå¤§çš„æ•°ï¼Œç¡®ä¿æœ€ç»ˆæˆåŠŸï¼‰
MAX_RETRIES = 50
# åˆå§‹é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œä¼šé€æ¸å¢åŠ 
RETRY_DELAY_BASE = 3
# æœ€å¤§é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
RETRY_DELAY_MAX = 30


class EventFetcher:
    def __init__(self, rpc_url: str, contracts: List[str], output_file: str):
        self.rpc_url = rpc_url
        self.contracts = contracts
        self.output_file = output_file
        self.session = requests.Session()

    def rpc_call(self, method: str, params: List[Any]) -> Dict:
        """å‘é€ JSON-RPC è¯·æ±‚"""
        payload = {
            "jsonrpc": "2.0",
            "method": method,
            "params": params,
            "id": 1
        }
        response = self.session.post(self.rpc_url, json=payload)
        response.raise_for_status()
        result = response.json()

        if "error" in result:
            raise Exception(f"RPC Error: {result['error']}")

        return result.get("result")

    def get_latest_block(self) -> int:
        """è·å–æœ€æ–°åŒºå—å·"""
        block_hex = self.rpc_call("eth_blockNumber", [])
        return int(block_hex, 16)

    def get_logs(self, from_block: int, to_block: int, contract: str, batch_info: str = "") -> List[Dict]:
        """è·å–æŒ‡å®šåŒºå—èŒƒå›´çš„äº‹ä»¶æ—¥å¿—ï¼ˆå¸¦æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ï¼‰"""
        topics_list = list(EVENT_TOPICS.values())

        params = [{
            "fromBlock": hex(from_block),
            "toBlock": hex(to_block),
            "address": contract,
            "topics": [topics_list]  # OR æŸ¥è¯¢æ‰€æœ‰å…³æ³¨çš„äº‹ä»¶
        }]

        # æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
        for attempt in range(MAX_RETRIES):
            try:
                logs = self.rpc_call("eth_getLogs", params)
                if attempt > 0:
                    print(f"      âœ“ é‡è¯•æˆåŠŸï¼")
                return logs if logs else []
            except Exception as e:
                if attempt < MAX_RETRIES - 1:
                    # æŒ‡æ•°é€€é¿ï¼šç­‰å¾…æ—¶é—´é€æ¸å¢åŠ 
                    wait_time = min(RETRY_DELAY_BASE * (2 ** attempt), RETRY_DELAY_MAX)
                    print(f"      âš  {batch_info} å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")
                    print(f"      â³ ç­‰å¾… {wait_time:.0f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"      âœ— {batch_info} é‡è¯• {MAX_RETRIES} æ¬¡åä»å¤±è´¥ï¼Œè¯·æ£€æŸ¥èŠ‚ç‚¹")
                    raise e

    def identify_event(self, log: Dict) -> str:
        """è¯†åˆ«äº‹ä»¶ç±»å‹"""
        if not log.get("topics"):
            return "Unknown"

        topic0 = log["topics"][0]
        for event_name, topic_hash in EVENT_TOPICS.items():
            if topic0.lower() == topic_hash.lower():
                return event_name

        return "Unknown"

    def load_existing_data(self) -> Dict:
        """åŠ è½½å·²å­˜åœ¨çš„æ•°æ®"""
        try:
            with open(self.output_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                "metadata": {
                    "contracts": self.contracts,
                    "last_update": None,
                    "total_events": 0,
                    "block_range": {"from": None, "to": None}
                },
                "events": []
            }

    def save_data(self, data: Dict):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def fetch_events(self, days: int):
        """è·å–æŒ‡å®šå¤©æ•°çš„äº‹ä»¶æ•°æ®"""
        print(f"å¼€å§‹è·å–æœ€è¿‘ {days} å¤©çš„äº‹ä»¶æ•°æ®...")
        print(f"RPC: {self.rpc_url}")
        print(f"åˆçº¦æ•°é‡: {len(self.contracts)}")
        print(f"ç›‘æ§äº‹ä»¶: {', '.join(EVENT_TOPICS.keys())}")
        print("=" * 60)

        # è·å–æœ€æ–°åŒºå—
        latest_block = self.get_latest_block()
        print(f"æœ€æ–°åŒºå—: {latest_block}")

        # è®¡ç®—èµ·å§‹åŒºå—
        total_blocks = days * BLOCKS_PER_DAY
        start_block = latest_block - total_blocks
        print(f"æŸ¥è¯¢èŒƒå›´: {start_block} -> {latest_block} (å…± {total_blocks} ä¸ªåŒºå—)")
        print("=" * 60)

        # åŠ è½½å·²æœ‰æ•°æ®
        data = self.load_existing_data()
        existing_event_count = len(data["events"])

        # æŒ‰å¤©åˆ†æ‰¹æŸ¥è¯¢
        all_new_events = []
        current_block = start_block

        for day in range(days):
            day_start_block = current_block
            day_end_block = min(current_block + BLOCKS_PER_DAY - 1, latest_block)

            print(f"\n[ç¬¬ {day + 1}/{days} å¤©] æŸ¥è¯¢åŒºå— {day_start_block} -> {day_end_block}")

            day_events = []

            # æŒ‰åˆçº¦å¾ªç¯
            for contract_idx, contract in enumerate(self.contracts):
                contract_events = []
                batch_current = day_start_block
                batch_count = 0
                batch_success = 0
                batch_failed = 0

                # å°†ä¸€å¤©çš„åŒºå—åˆ†æ‰¹æŸ¥è¯¢ï¼ˆæ¯æ‰¹æœ€å¤š MAX_BLOCKS_PER_QUERY ä¸ªåŒºå—ï¼‰
                while batch_current <= day_end_block:
                    batch_from = batch_current
                    batch_to = min(batch_current + MAX_BLOCKS_PER_QUERY - 1, day_end_block)
                    batch_count += 1

                    batch_info = f"åˆçº¦ {contract[:10]}... æ‰¹æ¬¡ {batch_count}/{(day_end_block - day_start_block + 1 + MAX_BLOCKS_PER_QUERY - 1) // MAX_BLOCKS_PER_QUERY} ({batch_from}-{batch_to})"

                    try:
                        logs = self.get_logs(batch_from, batch_to, contract, batch_info)

                        # å¤„ç†æ—¥å¿—
                        for log in logs:
                            event_type = self.identify_event(log)
                            event_data = {
                                "contract": contract,
                                "event_type": event_type,
                                "block_number": int(log["blockNumber"], 16),
                                "transaction_hash": log["transactionHash"],
                                "log_index": int(log["logIndex"], 16),
                                "topics": log["topics"],
                                "data": log["data"],
                                "removed": log.get("removed", False)
                            }
                            contract_events.append(event_data)

                        batch_success += 1
                        # æ˜¾ç¤ºè¿›åº¦ï¼ˆåªåœ¨æœ‰å¤šä¸ªæ‰¹æ¬¡æ—¶æ˜¾ç¤ºï¼‰
                        total_batches = (day_end_block - day_start_block + 1 + MAX_BLOCKS_PER_QUERY - 1) // MAX_BLOCKS_PER_QUERY
                        if total_batches > 1:
                            print(f"    âœ“ {batch_info}: {len(logs)} ä¸ªäº‹ä»¶")

                    except Exception as e:
                        batch_failed += 1
                        print(f"    âœ— {batch_info} æœ€ç»ˆå¤±è´¥ï¼Œç¨‹åºå°†ç»ˆæ­¢")
                        raise e

                    # é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(REQUEST_DELAY)

                    batch_current = batch_to + 1

                day_events.extend(contract_events)
                success_rate = (batch_success / batch_count * 100) if batch_count > 0 else 0
                status_emoji = "âœ“" if batch_failed == 0 else "âš "
                print(f"  {status_emoji} åˆçº¦ {contract_idx + 1}/{len(self.contracts)} ({contract[:10]}...): {len(contract_events)} ä¸ªäº‹ä»¶ (æˆåŠŸ {batch_success}/{batch_count} æ‰¹, {success_rate:.1f}%)")

            all_new_events.extend(day_events)
            print(f"  âœ“ ç¬¬ {day + 1} å¤©å…±è·å– {len(day_events)} ä¸ªäº‹ä»¶")

            # æ¯å¤©ä¿å­˜ä¸€æ¬¡
            data["events"].extend(day_events)
            data["metadata"]["last_update"] = datetime.now().isoformat()
            data["metadata"]["total_events"] = len(data["events"])
            data["metadata"]["block_range"]["from"] = start_block
            data["metadata"]["block_range"]["to"] = day_end_block
            self.save_data(data)
            print(f"  ğŸ’¾ å·²ä¿å­˜åˆ° {self.output_file} (ç´¯è®¡: {len(data['events'])} ä¸ªäº‹ä»¶)")

            current_block = day_end_block + 1

            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)

        # ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("âœ… æ•°æ®è·å–å®Œæˆï¼")
        print(f"æœ¬æ¬¡æ–°å¢äº‹ä»¶: {len(all_new_events)} ä¸ª")
        print(f"æ€»äº‹ä»¶æ•°: {len(data['events'])} ä¸ª (ä¹‹å‰: {existing_event_count})")

        # æŒ‰äº‹ä»¶ç±»å‹ç»Ÿè®¡
        event_type_counts = {}
        for event in data["events"]:
            event_type = event["event_type"]
            event_type_counts[event_type] = event_type_counts.get(event_type, 0) + 1

        print("\näº‹ä»¶ç±»å‹ç»Ÿè®¡:")
        for event_type, count in sorted(event_type_counts.items()):
            print(f"  {event_type}: {count} ä¸ª")

        print(f"\næ•°æ®å·²ä¿å­˜è‡³: {self.output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("PancakeSwap Prediction åˆçº¦äº‹ä»¶è·å–å·¥å…·")
    print("=" * 60)

    # è·å–ç”¨æˆ·è¾“å…¥
    try:
        days = int(input("\nè¯·è¾“å…¥è¦è·å–çš„å¤©æ•° (ä¾‹å¦‚: 7, 30, 90): ").strip())
        if days <= 0:
            print("âŒ å¤©æ•°å¿…é¡»å¤§äº 0")
            return
    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        return

    # åˆ›å»ºè·å–å™¨å¹¶æ‰§è¡Œ
    fetcher = EventFetcher(RPC_URL, CONTRACTS, OUTPUT_FILE)

    try:
        fetcher.fetch_events(days)
    except KeyboardInterrupt:
        print("\n\nâš  ç”¨æˆ·ä¸­æ–­ï¼Œæ•°æ®å·²ä¿å­˜è‡³æœ€åä¸€æ¬¡æ›´æ–°")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
